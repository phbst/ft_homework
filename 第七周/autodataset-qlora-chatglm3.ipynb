{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel,BitsAndBytesConfig\n",
    "from transformers import Trainer,TrainingArguments\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, TaskType, LoraConfig\n",
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "#配置一下全局超参数\n",
    "base_model_path=\"/mnt/data/chatglm3-6b-model\"\n",
    "train_data_path=\"/mnt/workspace/datasets.csv\"\n",
    "seed=42\n",
    "max_inputs=512\n",
    "max_outputs=1536\n",
    "lora_rank=16\n",
    "lora_dropout=0.05\n",
    "lora_alpha=32\n",
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#加载数据\n",
    "\n",
    "dataset=load_dataset(\"csv\",data_files=train_data_path)\n",
    "\n",
    "#加载分词器\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(base_model_path,trust_remote_code=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7fc0958554248b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenizer_function(example, tokenizer,ignore_lable_id=-100):\n",
    "    question=example[\"man\"]\n",
    "    answer=example[\"wemen\"]\n",
    "    q_ids=tokenizer.encode(question,add_special_tokens=False)\n",
    "    a_ids=tokenizer.encode(answer,add_special_tokens=False)\n",
    "    if len(q_ids)>max_inputs-2:\n",
    "        q_ids=q_ids[:max_inputs-2]\n",
    "    if len(a_ids)>max_outputs-1:\n",
    "        a_ids=a_ids[:max_outputs-1]\n",
    "    inputs_ids=tokenizer.build_inputs_with_special_tokens(q_ids,a_ids)\n",
    "    question_length=len(q_ids)+2\n",
    "    inputs_labels=[ignore_lable_id]*question_length+inputs_ids[question_length:]\n",
    "    return {\"input_ids\":inputs_ids,\"labels\":inputs_labels}\n",
    "\n",
    "\n",
    "tokenized_dataset=dataset[\"train\"].map(lambda example:tokenizer_function(example,tokenizer),batched=False,remove_columns=[\"wemen\",\"man\"])\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=seed)\n",
    "tokenized_dataset = tokenized_dataset.flatten_indices()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a0b502990944890"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Data_Collector:\n",
    "    def __init__(self,pad_token_id:int,max_length:int=2048,ignore_lable_id:int=-100):\n",
    "        self.pad_token_id=pad_token_id\n",
    "        self.max_length=max_length\n",
    "        self.ignore_lable_id=ignore_lable_id\n",
    "    def __call__(self,batch_data):\n",
    "        len_list=[len(i[\"input_ids\"]) for i in batch_data]\n",
    "        batch_max_len=max(len_list)\n",
    "        input_ids,labels=[],[]\n",
    "        for len_of_d,d in sorted(zip(len_list,batch_data),key=lambda x:-x[0]):\n",
    "            pad_len=batch_max_len-len_of_d\n",
    "            input_id=d[\"input_ids\"]+[self.pad_token_id]*pad_len\n",
    "            lable=d[\"labels\"]+[self.ignore_lable_id]*pad_len\n",
    "            if batch_max_len>self.max_length:\n",
    "                input_id=input_ids[:self.max_length]\n",
    "                label=lable[:self.max_length]\n",
    "            input_ids.append(torch.LongTensor(input_id))\n",
    "            labels.append(torch.LongTensor(lable))\n",
    "        input_ids=torch.stack(input_ids)\n",
    "        labels=torch.stack(labels)\n",
    "        return {\"input_ids\":input_ids,\"labels\":labels}\n",
    "data_collector=Data_Collector(pad_token_id=tokenizer.pad_token_id)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9192250acab59cc6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "base_model=AutoModel.from_pretrained(base_model_path,quantization_config=q_config,device_map=\"auto\",trust_remote_code=True)\n",
    "base_model.supports_gradient_checkpointing = True\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "kbit_model=prepare_model_for_kbit_training(base_model)\n",
    "target_model=TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['chatglm']\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    target_modules=target_model,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "qlora_model=get_peft_model(kbit_model,lora_config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20c62c6b319a91c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_dir=\"phb/chatglm3-ft\"\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=True\n",
    ")\n",
    "trainer=Trainer(\n",
    "    model=qlora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collector\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d992d586adbf049"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74e3e4ab910a0d8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "622deb10b9902151"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,BitsAndBytesConfig\n",
    "from peft import PeftModel,PeftConfig\n",
    "base_model_path=\"/mnt/data/chatglm3-6b-model\"\n",
    "ft_model_path=\"/mnt/workspace/phb/chatglm3-ft\"\n",
    "q_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "base_model=AutoModel.from_pretrained(base_model_path,quantization_config=q_config,device_map=\"auto\",trust_remote_code=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained(base_model_path)\n",
    "config=PeftConfig.from_pretrained(ft_model_path)\n",
    "model=PeftModel.from_pretrained(base_model,ft_model_path)\n",
    "\n",
    "def compare_base_to_ft(q):\n",
    "    base_response,base_history=base_model.chat(tokenizer,q)\n",
    "\n",
    "    inputs=tokenizer(q,return_tensors=\"pt\").to(0)\n",
    "    ft_outputs=model.generate(**inputs)\n",
    "    ft_response=tokenizer.decode(ft_outputs[0],skip_special_tokens=True)\n",
    "    print(\"问题：{}\\n\".format(q))\n",
    "    print(\"basic：{}\".format(base_response))\n",
    "    print(\"ft:\"+ft_response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8422005ae62035aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "936b851e2f985a8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
