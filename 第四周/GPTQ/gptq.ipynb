{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b750f6679a3f378",
   "metadata": {},
   "source": [
    "## 下载模型和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f096e6d17479ef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-29T07:53:57.759463800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# 设置 HF_ENDPOINT 环境变量\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "def download_model(model_name):\n",
    "    try:\n",
    "        subprocess.run(['huggingface-cli', 'download', '--resume-download', model_name,'--local-dir',model_name], check=True)\n",
    "        print(f\"Model '{model_name}' downloaded successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading model '{model_name}': {e}\")\n",
    "\n",
    "def download_dataset(dataset_name):\n",
    "    try:\n",
    "        subprocess.run(['huggingface-cli', 'download', '--resume-download','--repo-type','dataset',dataset_name,\"--local-dir\",dataset_name], check=True)\n",
    "        print(f\"Dataset '{dataset_name}' downloaded successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading dataset '{dataset_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d61ef-a8f5-4ba0-b144-1aabde2bf495",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813918558beffb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T08:08:13.229049500Z",
     "start_time": "2024-01-29T08:08:13.222598600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_model('facebook/opt-6.7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34e404f1d1c007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T08:08:58.099784Z",
     "start_time": "2024-01-29T08:08:58.085529300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d28cffa2f116df8",
   "metadata": {},
   "source": [
    "## 加载分词器，配置量化超参数，加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98fc68e6fe8352f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T08:22:08.125036Z",
     "start_time": "2024-01-29T08:22:04.637244Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:20:49.416269Z",
     "iopub.status.busy": "2024-02-16T07:20:49.415344Z",
     "iopub.status.idle": "2024-02-16T07:20:49.420308Z",
     "shell.execute_reply": "2024-02-16T07:20:49.419330Z",
     "shell.execute_reply.started": "2024-02-16T07:20:49.416226Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GPTQConfig, AutoTokenizer\n",
    "import torch\n",
    "model_id = \"/mnt/data/opt-6.7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea1c2043b2bad5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T08:23:59.469276900Z",
     "start_time": "2024-01-29T08:23:59.453222Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:20:49.849981Z",
     "iopub.status.busy": "2024-02-16T07:20:49.849043Z",
     "iopub.status.idle": "2024-02-16T07:20:49.854221Z",
     "shell.execute_reply": "2024-02-16T07:20:49.853317Z",
     "shell.execute_reply.started": "2024-02-16T07:20:49.849937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config=GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=[\"Hello! How can I assist you today?\"],\n",
    "    desc_act=False\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcf45de6cf3b6d",
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32600a9a-780c-47eb-9c31-1f6d5f40b8d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981a0e9f268f183",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:20:51.091924Z",
     "iopub.status.busy": "2024-02-16T07:20:51.091176Z",
     "iopub.status.idle": "2024-02-16T07:20:51.679035Z",
     "shell.execute_reply": "2024-02-16T07:20:51.678081Z",
     "shell.execute_reply.started": "2024-02-16T07:20:51.091885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0b44ec4138d99b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:20:51.681500Z",
     "iopub.status.busy": "2024-02-16T07:20:51.680901Z",
     "iopub.status.idle": "2024-02-16T07:22:15.539331Z",
     "shell.execute_reply": "2024-02-16T07:22:15.537748Z",
     "shell.execute_reply.started": "2024-02-16T07:20:51.681452Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:11<00:00, 35.95s/it]\n",
      "Quantizing model.decoder.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:01<00:09,  1.90s/it]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:03<00:07,  1.90s/it]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:05<00:05,  1.90s/it]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:07<00:03,  1.90s/it]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:09<00:01,  1.92s/it]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :   0%|          | 0/32 [00:09<?, ?it/s]  \u001B[A\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.78 GiB of which 931.75 MiB is free. Process 13700 has 14.87 GiB memory in use. Of the allocated memory 13.54 GiB is allocated by PyTorch, and 199.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m quant_model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    565\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    572\u001B[0m )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3780\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mmain_input_name \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3779\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe can only quantize pure text model.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 3780\u001B[0m \u001B[43mquantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3781\u001B[0m config\u001B[38;5;241m.\u001B[39mquantization_config \u001B[38;5;241m=\u001B[39m GPTQConfig\u001B[38;5;241m.\u001B[39mfrom_dict_optimum(quantizer\u001B[38;5;241m.\u001B[39mto_dict())\n\u001B[1;32m   3782\u001B[0m model\u001B[38;5;241m.\u001B[39m_is_quantized_training_enabled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/optimum/gptq/quantizer.py:500\u001B[0m, in \u001B[0;36mGPTQQuantizer.quantize_model\u001B[0;34m(self, model, tokenizer)\u001B[0m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# update Hessian for each layer in subset_layers thanks to the hook\u001B[39;00m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(dataset)):\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;66;03m# the args are already on the gpu\u001B[39;00m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;66;03m# don't need to store the output\u001B[39;00m\n\u001B[0;32m--> 500\u001B[0m     \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlayer_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mlayer_input_kwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# remove hook\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m handles:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:576\u001B[0m, in \u001B[0;36mOPTDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    573\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(hidden_states)\n\u001B[1;32m    574\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_fn(hidden_states)\n\u001B[0;32m--> 576\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    577\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(hidden_states, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    579\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m (residual \u001B[38;5;241m+\u001B[39m hidden_states)\u001B[38;5;241m.\u001B[39mview(hidden_states_shape)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1581\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1579\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, args, kwargs, result)\n\u001B[1;32m   1580\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1581\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hook_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1584\u001B[0m     result \u001B[38;5;241m=\u001B[39m hook_result\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/optimum/gptq/quantizer.py:490\u001B[0m, in \u001B[0;36mGPTQQuantizer.quantize_model.<locals>.add_batch.<locals>.tmp\u001B[0;34m(_, input, output)\u001B[0m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtmp\u001B[39m(_, \u001B[38;5;28minput\u001B[39m, output):\n\u001B[0;32m--> 490\u001B[0m     \u001B[43mgptq\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/auto_gptq/quantization/gptq.py:60\u001B[0m, in \u001B[0;36mGPTQ.add_batch\u001B[0;34m(self, inp, out)\u001B[0m\n\u001B[1;32m     58\u001B[0m inp \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnsamples) \u001B[38;5;241m*\u001B[39m inp\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# self.H += 2 / self.nsamples * inp.matmul(inp.t())\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mH \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43minp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.78 GiB of which 931.75 MiB is free. Process 13700 has 14.87 GiB memory in use. Of the allocated memory 13.54 GiB is allocated by PyTorch, and 199.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f3248-ff44-4327-86f2-14f5b2847f27",
   "metadata": {},
   "source": [
    "## 检查模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316495780560157",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-02-16T07:22:15.540271Z",
     "iopub.status.idle": "2024-02-16T07:22:15.540650Z",
     "shell.execute_reply": "2024-02-16T07:22:15.540500Z",
     "shell.execute_reply.started": "2024-02-16T07:22:15.540483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb717f-11c2-4ca3-bd53-8595e972b354",
   "metadata": {},
   "source": [
    "# 显存不够了，我就直接改用125m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07448b99-3db8-4382-84f8-002e4d6d8e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T07:24:58.512635Z",
     "iopub.status.busy": "2024-02-16T07:24:58.511676Z",
     "iopub.status.idle": "2024-02-16T07:25:45.897711Z",
     "shell.execute_reply": "2024-02-16T07:25:45.896756Z",
     "shell.execute_reply.started": "2024-02-16T07:24:58.512597Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.38it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :   8%|▊         | 1/12 [00:03<00:35,  3.24s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.38it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  17%|█▋        | 2/12 [00:06<00:32,  3.24s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.76it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.75it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.76it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.38it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  25%|██▌       | 3/12 [00:09<00:29,  3.24s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.37it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  33%|███▎      | 4/12 [00:12<00:25,  3.25s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.38it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  42%|████▏     | 5/12 [00:16<00:22,  3.24s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.39it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  50%|█████     | 6/12 [00:19<00:19,  3.24s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.39it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  58%|█████▊    | 7/12 [00:22<00:16,  3.23s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.80it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.39it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  67%|██████▋   | 8/12 [00:25<00:12,  3.23s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.82it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.39it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  75%|███████▌  | 9/12 [00:29<00:09,  3.22s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.82it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.81it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.40it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  83%|████████▎ | 10/12 [00:32<00:06,  3.22s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.75it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.76it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.38it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks :  92%|█████████▏| 11/12 [00:35<00:03,  3.23s/it]\n",
      "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      "Quantizing layers inside the block:  17%|█▋        | 1/6 [00:00<00:01,  2.77it/s]\u001B[A\n",
      "Quantizing layers inside the block:  33%|███▎      | 2/6 [00:00<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  50%|█████     | 3/6 [00:01<00:01,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s]\u001B[A\n",
      "Quantizing layers inside the block:  83%|████████▎ | 5/6 [00:01<00:00,  2.79it/s]\u001B[A\n",
      "Quantizing layers inside the block: 100%|██████████| 6/6 [00:03<00:00,  1.39it/s]\u001B[A\n",
      "Quantizing model.decoder.layers blocks : 100%|██████████| 12/12 [00:38<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"/mnt/data/opt-125m\"\n",
    "config=GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=[\"Hello! How can I assist you today?\"],\n",
    "    desc_act=False\n",
    "    \n",
    ")\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13cf544f771a8c",
   "metadata": {},
   "source": [
    "## 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6f868de065bc94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T08:51:15.351388600Z",
     "start_time": "2024-01-29T08:51:15.332075600Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:26:05.603037Z",
     "iopub.status.busy": "2024-02-16T07:26:05.602265Z",
     "iopub.status.idle": "2024-02-16T07:26:05.607545Z",
     "shell.execute_reply": "2024-02-16T07:26:05.606602Z",
     "shell.execute_reply.started": "2024-02-16T07:26:05.602996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text=\"hello,can you introduce yourself?\"\n",
    "input=tokenizer(text,return_tensors='pt').to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05384e8b-a18e-4425-88e0-65dfd84dd9d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T07:26:07.961544Z",
     "iopub.status.busy": "2024-02-16T07:26:07.960796Z",
     "iopub.status.idle": "2024-02-16T07:26:07.967898Z",
     "shell.execute_reply": "2024-02-16T07:26:07.967161Z",
     "shell.execute_reply.started": "2024-02-16T07:26:07.961509Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 42891,     6,  7424,    47,  6581,  2512,   116]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e24c7459c50d9d9",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-16T07:26:16.805930Z",
     "iopub.status.busy": "2024-02-16T07:26:16.805170Z",
     "iopub.status.idle": "2024-02-16T07:26:17.037087Z",
     "shell.execute_reply": "2024-02-16T07:26:17.036333Z",
     "shell.execute_reply.started": "2024-02-16T07:26:16.805891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,can you introduce yourself?\n",
      "I am am am am am am am am am am am am am am am am am am\n"
     ]
    }
   ],
   "source": [
    "output=quant_model.generate(**input,max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 有一点问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "825d3673d6b63e85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b167547cf9f8d62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
