# 作业

**单项选择题**

1. n-gram 模型中的“n”通常代表什么？
    - A. 神经网络的层数
    - B. 词汇表的大小
    - C. 上下文中的词数
    - D. 可选的参数数量

    答案: C. 上下⽂中的词数
    解释: 在n-gram模型中，“n”代表上下⽂中的词数。例如，⼀个3-gram（或trigram）模型会在预测下⼀个词时考虑前两个词。

2. 最大似然估计在 n-gram 模型中的主要目的是什么？
    - A. 最小化错误率
    - B. 优化训练速度
    - C. 预测下一个词的概率
    - D. 减少计算复杂度

    2. 答案: C. 预测下⼀个词的概率
    解释: 在n-gram模型中，最⼤似然估计⽤于计算⼀个词序列出现的概率，从⽽预测下⼀个最可能出的词。

3. Neural Probabilistic Language Model 的主要创新是什么？
    - A. 使用深度学习方法
    - B. 引入了 n-gram 概念
    - C. 将词映射到高维空间
    - D. 使用最大似然估计

    3. 答案: A. 使⽤深度学习⽅法
    解释: 神经概率语⾔模型的主要创新之⼀是利⽤深度学习技术来处理语⾔建模问题。

4. Word2Vec 的主要特点是什么？
    - A. 能够处理非结构化数据
    - B. 将词转换为密集向量表示
    - C. 基于循环神经网络
    - D. 使用了自编码器
    1. 答案: B. 将词转换为密集向量表示
    解释: Word2Vec的主要特点是将词转换为密集的向量表示，这些向量在⾼维空间中能够表示词与词之间的语义关系。

5. 在 Word2Vec 中，CBOW 和 Skip-Gram 模型的主要区别是什么？
    - A. CBOW 预测上下文，而 Skip-Gram 预测中心词
    - B. Skip-Gram 使用深度学习，而 CBOW 不使用
    - C. CBOW 对高频词更有效，而 Skip-Gram 对低频词更有效
    - D. Skip-Gram 比 CBOW 计算复杂度更高
    1. 答案: C. CBOW对⾼频词更有效，⽽Skip-Gram对低频词更有效
    解释: CBOW模型在处理⾼频词时表现更好，因为它平均了上下⽂中所有词的影响。⽽Skip-Gram模型对于低频词汇和更具挑战的短语表现更好，因为它对每个上下⽂词都⽣成独⽴的样本

6. 注意力机制（Attention）的主要用途是什么？
    - A. 优化模型训练速度
    - B. 提高模型准确率
    - C. 选择重要的信息并忽略不相关的信息
    - D. 改进模型的可解释性
    1. 答案: C. 选择重要的信息并忽略不相关的信息
    解释: 注意⼒机制允许模型在处理信息时，集中于当前任务最相关的部分，从⽽提⾼处理效率和性能。
7. Transformer 模型是基于什么理论构建的？
    - A. 递归神经网络（RNN）
    - B. 卷积神经网络（CNN）
    - C. 注意力机制（Attention）
    - D. 自组织映射（SOM）
    1. 答案: C. 注意⼒机制（Attention）
    解释: Transformer模型的核⼼是基于注意⼒机制，这使得它能够更有效地处理序列数据，特别是在⻓距离依赖⽅⾯。

8. GPT 和 BERT 的主要区别是什么？
    - A. GPT 是基于 Transformer 的，而 BERT 不是
    - B. BERT 是基于 Transformer 的，而 GPT 不是
    - C. GPT 使用了单向自注意力，而 BERT 使用了双向自注意力
    - D. GPT 和 BERT 在基本结构上没有区别
    1. 答案: C. GPT使⽤了单向⾃注意⼒，⽽BERT使⽤了双向⾃注意⼒
    解释: GPT采⽤的是单向（或顺序）⾃注意⼒机制，这意味着它在⽣成⽂本时只考虑之前的词，⽽BERT采⽤双向⾃注意⼒机制，同时考虑上下⽂中的前后词。

9. Transformer 模型是如何解决长距离依赖问题的？
    - A. 通过递归神经网络（RNN）
    - B. 通过卷积神经网络（CNN）
    - C. 通过注意力机制（Attention）
    - D. 通过自组织映射（SOM）
    1. 答案: C. 通过注意⼒机制（Attention）
    解释: Transformer通过注意⼒机制处理输⼊，这使得它能够有效地捕捉⻓距离依赖关系，不受序列⻓度的限制

10. GPT 主要用于哪种类型的任务？
    - A. 分类任务
    - B. 回归任务
    - C. 生成任务
    - D. 聚类任务
    1. 答案: C. ⽣成任务
    解释: GPT（Generative Pretrained Transformer）主要设计⽤于⽣成任务，如⽂本⽣成，由于其单向
    ⾃注意⼒机制，它在⽣成连贯和相关的⽂本⽅⾯表现出⾊。

11. 以下哪项是 BERT 的主要创新之处？
    - A. 引入了自注意力机制
    - B. 使用了双向自注意力机制
    - C. 提出了新的优化算法
    - D. 突破了模型大小的限制
    1. 答案: B. 使⽤了双向⾃注意⼒机制
    解释: BERT的主要创新之处在于它使⽤了双向⾃注意⼒机制，这使得模型能够更全⾯地理解输⼊数据的上下⽂

12. 在 Transformer 模型中，自注意力机制的主要作用是什么？
    - A. 加速模型训练
    - B. 识别输入中的关键信息
    - C. 生成高质量的词嵌入
    - D. 提高模型的鲁棒性
    1. 答案: B. 识别输⼊中的关键信息
    解释: Transformer中的⾃注意⼒机制能够识别输⼊数据中的关键信息，并将注意⼒集中在这些信息
    上，从⽽提⾼模型的性能。

13. 基于 Transformer 的模型，如 GPT 和 BERT，主要适用于哪些任务？
    - A. 图像识别
    - B. 自然语言处理
    - C. 语音识别
    - D. 强化学习
    1. 答案: B. ⾃然语⾔处理
    解释: 基于Transformer的模型，如GPT和BERT，主要适⽤于⾃然语⾔处理任务，包括⽂本分类、问答
    系统、⽂本⽣成等。

14. 注意力机制最早是在哪个领域得到应用的？
    - A. 计算机视觉
    - B. 语音识别
    - C. 自然语言处理
    - D. 推荐系统
    1. 答案: C. ⾃然语⾔处理
    解释: 注意⼒机制最初是在⾃然语⾔处理领域得到应⽤的，⽤于提⾼序列模型的性能，特别是在处理⻓距离依赖⽅⾯

15. GPT-2 相比于 GPT-1 主要改进了哪些方面？
    - A. 只改进了训练算法
    - B. 扩大了模型的规模和数据处理能力
    - C. 引入了新的神经网络架构
    - D. 专注于改进图像识别能力
    1. 答案: B. 扩⼤了模型的规模和数据处理能⼒
    解释: GPT-2 相⽐于 GPT-1 的主要改进在于显著增加了模型的规模（即参数的数量）和处理更⼤量数据的能⼒。这些改进使得 GPT-2 能够更有效地处理更复杂的语⾔处理任务。

16. GPT-3 的主要优势在于其能力的哪个方面？
    - A. 更小的模型规模和更快的训练速度
    - B. 更高的能效和更低的计算成本
    - C. 处理更复杂的任务和更大规模的数据
    - D. 更准确的语音识别能力
    1. 答案: C. 处理更复杂的任务和更⼤规模的数据
    解释: GPT-3 的主要优势在于它的⼤规模和强⼤的处理能⼒，使其能够处理更复杂的任务和分析更⼤规模的数据。s它拥有⽐前代模型更多的参数，使得它在各种语⾔任务上的表现更加出⾊。


关于课程中未完全理解的概念，建议使用 ChatGPT 帮助解答，示例如下：

**Q1：**是不是说 bert 模型必须要做下游的任务，写标签？因为它是一个理解语义的模型。而 GPT 是生成式可以直接使用预训练模型。

**A1：**是的，BERT 模型通常用于下游任务并且需要标签数据。这是因为 BERT 是一个基于 Transformer 的模型，被设计来理解语言而不是直接生成文本。在训练过程中，BERT 模型通过预测句子中被隐去的词来学习语义信息。但为了在特定任务 (如情感分析、问题回答等) 上表现良好，通常需要在对应的有标签数据集上进行微调。

GPT 系列模型则不同，它们是生成式模型，这意味着 GPT 可以直接用于生成文本。GPT 模型在训练时学习预测下一个词，因此在训练完成后就可以直接用于文本生成，不一定需要针对特定任务的微调，尽管在某些情况下微调可以提升性能。

**Q2：**word2vec 的处理对象是句子，还是经过分词的 token？

**A2：**Word2Vec 的处理对象是词 (words)，而不是整个句子。它通过上下文来学习词的向量表示。在使用 Word2Vec 时，通常首先需要对文本进行分词，将句子分解为独立的词。然后，Word2Vec 模型会使用这些词及其上下文来学习词向量。

**Q3：**encoder 里的自注意力机制，与 decoder 里的注意力机制有什么区别？

**A3：**在 Transformer 模型中，encoder 和 decoder 的注意力机制有一些关键区别:

Encoder 的自注意力机制: 在 encoder 中，每人词元只关注输入序列中的其他词元。这种自注意力机制允许模型理解输入序列内各个词元之间的关系 Decoder 的注意力机制: Decoder 包含两种注意力机制。第一种是自注意力，类似于 encoder 中的自注意力，但它被掩蔽以防止未来位置的信息泄露。第二种是交叉注意力 (cross-attention) ，decoder 的这一部分关注整个 encoder 的输出。这种机制允许 decoder 在生成每个词时考虑整个输入序列，从而在翻译或文本生成等任务中更有效地利用输入信息。

的⼤规模和强⼤的处理能⼒，使其能够处理更复杂的任务和分析更⼤规模的数据。s它拥有⽐前代模型更多的参数，使得它在各种语⾔任务上的表现更加出⾊。